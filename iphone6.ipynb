
from bs4 import BeautifulSoup
import requests
import re
import time
from __future__ import division, unicode_literals 
import codecs

HEADERS = ({'User-Agent':
            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',
            'Accept-Language': 'en-US, en;q=0.5'})


# This function allows you to access the URL and downloads the html file into your local 
def question1(URL,count):
    user_agent = {'User-agent': 'Mozilla/5.0'} 
    page = requests.get(URL, user_agent)
    doc = BeautifulSoup(page.content, "html.parser")
    with open("ebay_iphone6s_"+str(count)+".html", "w", encoding="utf-8") as f:
        f.write(str(doc))

# This function iterates through the first 10 pages of the search result. The parameter &_pgn allows you to set the page number
def question2(URL):
    for i in range(1,11):
        URL2 = URL +"&_pgn="+str(i)
        question1(URL2,i)
        time.sleep(10)    
    
# This function iterates through each html page and prints the URL of any sponsored products
def question3():
    for i in range(1,11):
        f=codecs.open("ebay_iphone6s_"+str(i)+".html", 'r', 'utf-8')
        doc= BeautifulSoup(f.read(), "html.parser")
        list_of_spon_items = doc.find_all("span", {"class":"s-item__title--tagblock__SPONSORED"})
        prodnum = 1
        for p in list_of_spon_items:
            spon_parent = p.parent.parent.findNext('a')
            url3 = str(spon_parent['href'])
            print("URL for Sponsored item_"+str(prodnum)+" On Page Number_"+str(i)+" is "+url3)
            print('\n')
            prodnum +=1


def main():
    try:
        question1("https://www.ebay.com/sch/i.html?&_nkw=iphone+6s","firstpage")
        question2("https://www.ebay.com/sch/i.html?_nkw=iphone+6s")
        question3()
        
    except Exception as e:
        print(e)


if __name__ == '__main__':
    main()
